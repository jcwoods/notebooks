{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Use of Bayes theorom and n-grams to classify (score) passwords.</h1>\n",
    "\n",
    "Bayes theory provides us with the formula:\n",
    "\n",
    "$$P(A|B)=\\frac{P(A)\\prod_{i=1}^n P(B|A)_i}{\\prod_{i=1}^n P(B)_i}$$\n",
    "\n",
    "Since we are interested in scoring passwords based on content of n-grams, we can assign meaning:\n",
    "\n",
    "<ul>\n",
    "<li>$P(A)$ = probability that a word is valid</li>\n",
    "<li>$P(B)$ = probability that a word - valid or not! - contains a (set of) digrams</li>\n",
    "<li>$P(A|B)$ = probability that a word is valid given it contains a particular (set of) digrams.</li>\n",
    "<li>$P(B|A)$ = probability that a particular (set of) digrams is contained in a valid word</li>\n",
    "</ul>\n",
    "\n",
    "And this is probably where my entire theory unravels... by substitution, we can arrive at:\n",
    "\n",
    "$$P(wordvalid|setofngrams)=\\frac{P(wordvalid)\\prod_{i=1}^n P(containsngram_i|wordvalid)_i}{\\prod_{i=1}^n P(containsngram_i)_i}$$\n",
    "\n",
    "<h2>The Alphabet</h2>\n",
    "To begin our analysis, we begin with an alphabet 'A' (a list) consisting of 'a' (an integer) characters:\n",
    "\n",
    "<ul>\n",
    "<li>a = size of alphabet used to build words (int)</li>\n",
    "<li>A = alphabet (list)</li>\n",
    "</ul>\n",
    "\n",
    "For an alphabet with an 'a' of 3, A would be ['a', 'b', 'c'].\n",
    "\n",
    "<h2>The Words</h2>\n",
    "Building from the alphabet, we have two word lists:\n",
    "\n",
    "The first list is all known valid words.  We will refer to this as 'D' (a list), which contains 'd' distinct words.  This is the training data.\n",
    "\n",
    "The second list is all possible words which can be built from the alphabet given other constraints.  For a word of length $l_w$ (represented by the variable 'lw'), we can arrange the letters of the alphabet into $a^lw$ possible permutations.  We will call this list 'W', and it contains 'w' (an integer) entries.\n",
    "\n",
    "In most cases, 'W' will be difficult or impossible to compute fully.  By comparison, the dictionary (D) is a small subset of this list of possible words (W).\n",
    "\n",
    "<ul>\n",
    "<li>D = list of known valid words (the dictionary)</li>\n",
    "<li>d = the number of words in 'D'</li>\n",
    "<li>W = list of all possible words (the world)</li>\n",
    "<li>w = the number of words in 'W'</li>\n",
    "<li>$l_w$ or 'lw' is the length of a word built from 'A'\n",
    "</ul>\n",
    "\n",
    "<h2>N-Grams</h2>\n",
    "\n",
    "N-Grams are groupsings of 'n' adjacent letters taken from a word.  For instance, the set of 2-grams (more commonly called 'bigrams') from the word 'alpha' would be ['al', 'lp', 'ph', 'ha'].\n",
    "\n",
    "<ul>\n",
    "<li>$l_n$ (an integer) is the size of n-gram.  In the code, this is variable 'ln'.</li>\n",
    "<li>G (a list) is the set of all NGrams which can be built using alphabet 'A'\n",
    "<li>g (an integer) is the number of distinct NGrams in the list G.\n",
    "</ul>\n",
    "\n",
    "In processing the dictionary (D), ngram frequencies are collected for each word in a frequency table 'F':\n",
    "\n",
    "<ul>\n",
    "<li>F (a dict) is the frequency of ngrams from all words in the dictionary<li>\n",
    "</ul>\n",
    "\n",
    "Finally, for a word of length $l_w$, we can see that $g_w = l_w - l_n + 1$ ngrams can be extracted.\n",
    "\n",
    "<h2>The Math</h2>\n",
    "Returning to our equiation at the top of this frame,\n",
    "\n",
    "<ul>\n",
    "<li>$P(A) = d/w$.  Probability that a word is valid, this will be 'P_wv' in the code below.</li>\n",
    "<li>$P(B)$ = probability of a word (valid or not) containing a particular set of ngrams.</li>\n",
    "<li>$P(A|B)$ = the &#36;1,000,000 question!</li>\n",
    "<li>$P(B|A)$ = the product of all ngram probabilities, each taken from F.</li>\n",
    "</ul>\n",
    "\n",
    "Given that 'W' is impossible (or, more accurately, infeasible) to compute, building a frequency table for it as we have done with 'D' and 'F' will be equally difficult.  Still, these values are required to compute P(B).\n",
    "\n",
    "I also believe that this is the point at which my work begins to falter.\n",
    "\n",
    "Since W is all permutations of A, we have a possible work-around.  We know how many permutations should exist if we actually built W.  We will call this quantity 'w' (an integer).  We also know how many ngrams will be produced for a word of a given length (gw) and the number of ngrams which can be produced by the alphabet A (eg, g). Together,\n",
    "\n",
    "$$c = \\frac{g_w * w}{g}$$\n",
    "\n",
    "The use of the name 'c' here is arbitrary, but it represents a count of the number of times a random ngram appears across all of W.\n",
    "\n",
    "$$P(B) = (\\frac{c}{w})^{l_w}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set global variables\n",
    "\n",
    "# this is the length of word we're targeting\n",
    "lw = 6\n",
    "\n",
    "# this is the size of digram we're using.  2 is too short.  3 may be appropriate.\n",
    "ln = 3\n",
    "\n",
    "# this is the file which contains \"valid\" words.  Use a reasonable default.  See\n",
    "# https://wiki.skullsecurity.org/Passwords for a list of real-world passwords.\n",
    "dictionary = '/usr/share/dict/words'\n",
    "\n",
    "# defines the alphabet (valid characters).  Flip these booleans to True/False\n",
    "# and the alphabet will be constructed properly below.\n",
    "use_upper = True\n",
    "use_lower = True\n",
    "use_numer = False\n",
    "use_punct = False\n",
    "\n",
    "# enable verbose output in calculations (for debugging)\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ngrams per word: 4\n",
      "Words in universe (a = 52, lw = 6) = 19770609664\n",
      "Possible ngrams (a = 52, ln = 3) = 140608\n",
      "Using filter regex: ^[ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz]{6}$\n",
      "9146 words read\n"
     ]
    }
   ],
   "source": [
    "# Okay... we'll do all the setup in this cell.  Should be no need for\n",
    "# modifications here.\n",
    "\n",
    "from math import pow\n",
    "import re\n",
    "\n",
    "# create the alphabet\n",
    "A=''\n",
    "if use_upper is True: A += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "if use_lower is True: A += 'abcdefghijklmnopqrstuvwxyz'\n",
    "if use_numer is True: A += '0123456789'\n",
    "if use_punct is True: A += '!@#$%^&*.,?:\\-'  # note -- incomplete set!!  Be careful\n",
    "                                             # about breaking the regex below.\n",
    "\n",
    "a = int(len(A))       # size of the alphabet\n",
    "w = int(pow(a, lw))   # number of words in the universe (a^lw)\n",
    "g = int(pow(a, ln))   # number of possible ngrams built from A\n",
    "gw = ((lw - ln) + 1)  # number of ngrams in a word\n",
    "\n",
    "print('Number of ngrams per word: {}'.format(gw))\n",
    "print('Words in universe (a = {}, lw = {}) = {}'.format(a, lw, w))\n",
    "print('Possible ngrams (a = {}, ln = {}) = {}'.format(a, ln, g))\n",
    "\n",
    "## Build the dictionary\n",
    "\n",
    "# regex used to filter words (see output below after execution!)\n",
    "rtmp = '^[{}]{{{}}}$'.format(A, lw)\n",
    "regex = re.compile(rtmp)\n",
    "print('Using filter regex: ' + rtmp)\n",
    "\n",
    "D = []\n",
    "dictfile = open(dictionary, 'r')\n",
    "for line in dictfile:\n",
    "    word = line.strip()\n",
    "    if regex.match(word): D.append(word)\n",
    "\n",
    "d = len(D)\n",
    "print('{} words read'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ngrams extracted: 5167\n",
      "Possible ngrams not observed: 135441\n"
     ]
    }
   ],
   "source": [
    "## extract frequencies\n",
    "\n",
    "from ngram import NGram\n",
    "\n",
    "ng = NGram(N=ln)\n",
    "ngrams = dict()  # the dictionary containing count of all grams\n",
    "\n",
    "for word in D:\n",
    "    grams = list(ng.ngrams(word))  # only count unique ngrams\n",
    "    for gram in grams:\n",
    "        try:\n",
    "            ngrams[gram] = ngrams[gram] + 1\n",
    "        except KeyError:\n",
    "            ngrams[gram] = 1\n",
    "\n",
    "print('Unique ngrams extracted: {}'.format(len(ngrams) ))\n",
    "print('Possible ngrams not observed: {}'.format(g - len(ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word is: shaggy\n",
      "P(wv) = 9146 / 19770609664 = 0.00000046260586574899\n",
      "Ngrams with frequencies for shaggy\n",
      "  P(sha) = 43 / 9146 = 0.00470150885633063668\n",
      "  P(hag) = 2 / 9146 = 0.00021867483052700635\n",
      "  P(agg) = 21 / 9146 = 0.00229608572053356663\n",
      "  P(ggy) = 5 / 9146 = 0.00054668707631751580\n",
      "P(ng|wv) = 0.00000000000129051472\n",
      "P(ng) = (562432 / 19770609664)^4 = 0.00000000000000000065\n",
      "shaggy:             0.911537\n",
      "\n",
      "input word is: purple\n",
      "P(wv) = 9146 / 19770609664 = 0.00000046260586574899\n",
      "Ngrams with frequencies for purple\n",
      "  P(pur) = 25 / 9146 = 0.00273343538158757945\n",
      "  P(urp) = 6 / 9146 = 0.00065602449158101901\n",
      "  P(rpl) = 2 / 9146 = 0.00021867483052700635\n",
      "  P(ple) = 35 / 9146 = 0.00382680953422261105\n",
      "P(ng|wv) = 0.00000000000150059851\n",
      "P(ng) = (562432 / 19770609664)^4 = 0.00000000000000000065\n",
      "purple:             1.059927\n",
      "\n",
      "input word is: zaxbys\n",
      "P(wv) = 9146 / 19770609664 = 0.00000046260586574899\n",
      "Ngrams with frequencies for zaxbys\n",
      "  P(zax) = 0 / 9146 = 0.00000000000000000000\n",
      "  P(axb) = 0 / 9146 = 0.00000000000000000000\n",
      "  P(xby) = 0 / 9146 = 0.00000000000000000000\n",
      "  P(bys) = 1 / 9146 = 0.00010933741526350318\n",
      "P(ng|wv) = 0.00000000000000000000\n",
      "P(ng) = (562432 / 19770609664)^4 = 0.00000000000000000065\n",
      "zaxbys:             0.000000\n",
      "\n",
      "input word is: Latino\n",
      "P(wv) = 9146 / 19770609664 = 0.00000046260586574899\n",
      "Ngrams with frequencies for Latino\n",
      "  P(Lat) = 5 / 9146 = 0.00054668707631751580\n",
      "  P(ati) = 31 / 9146 = 0.00338945987316859824\n",
      "  P(tin) = 57 / 9146 = 0.00623223267001968111\n",
      "  P(ino) = 13 / 9146 = 0.00142138639842554122\n",
      "P(ng|wv) = 0.00000000001641440399\n",
      "P(ng) = (562432 / 19770609664)^4 = 0.00000000000000000065\n",
      "Latino:            11.594083\n",
      "\n",
      "input word is: queasy\n",
      "P(wv) = 9146 / 19770609664 = 0.00000046260586574899\n",
      "Ngrams with frequencies for queasy\n",
      "  P(que) = 25 / 9146 = 0.00273343538158757945\n",
      "  P(uea) = 3 / 9146 = 0.00032801224579050950\n",
      "  P(eas) = 30 / 9146 = 0.00328012245790509525\n",
      "  P(asy) = 4 / 9146 = 0.00043734966105401271\n",
      "P(ng|wv) = 0.00000000000128622729\n",
      "P(ng) = (562432 / 19770609664)^4 = 0.00000000000000000065\n",
      "queasy:             0.908509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def wordValid(inword):\n",
    "    if verbose is True: print('input word is: {0:s}'.format(inword))\n",
    "\n",
    "    P_wv = float(d) / float(w)  # probability of a word being valid\n",
    "    if verbose is True: print('P(wv) = {0:d} / {1:d} = {2:.20f}'.format(d, w, P_wv))\n",
    "\n",
    "    P_ng_wv = 1  # probability of a ngram sequence given word is valid (eg, in dictionary)\n",
    "    if verbose is True: print('Ngrams with frequencies for {0:s}'.format(inword))\n",
    "    for gram in list(ng.ngrams(inword)):\n",
    "        try:\n",
    "            occs = ngrams[gram]\n",
    "        except KeyError:\n",
    "            occs = 0\n",
    "            \n",
    "        if verbose is True: print('  P({0:s}) = {1:d} / {2:d} = {3:.20f}'.format(gram, occs, d, float(occs) / float(d)))\n",
    "        P_ng_wv *= (float(occs) / float(d))\n",
    "\n",
    "    if verbose is True: print('P(ng|wv) = {0:.20f}'.format(P_ng_wv))\n",
    "    \n",
    "    c = (gw * w) / g\n",
    "    P_ng = pow((float(c) / float(w)), gw)\n",
    "    if verbose is True: print('P(ng) = ({0:d} / {1:d})^{2:d} = {3:.20f}'.format(c, w, gw, P_ng))\n",
    "\n",
    "    P = (P_wv * P_ng_wv) / P_ng        \n",
    "    return P\n",
    "\n",
    "# a handful of sample test sets for various word lengths (may need adjustment!!)\n",
    "testsets = { 3: ['cac', 'bac', 'cab' ],\n",
    "             6: ['shaggy', 'purple', 'zaxbys', 'Latino', 'queasy' ],\n",
    "             8: ['AJNAUTH', 'onuraage', '1n6sd11y', 'MINNIVER', 'PFELDMAN', 'ATLANTA9', \n",
    "                 'erenberg', 'rnjoyhuh', 'GIS-MOLL', 'jenalynn', 'alalalal', 'F*&vsQ2r'] }\n",
    "\n",
    "testset = testsets[lw]\n",
    "\n",
    "for word in testset:\n",
    "    print('{0:s}: {1:20f}'.format(word, wordValid(word)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
